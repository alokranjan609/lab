# -*- coding: utf-8 -*-
"""lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nZoYuA22hk-kUck3HT94yY8nmwhxx6BY
"""

import pandas as pd

import nltk
nltk.download('punkt_tab')

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

df = pd.read_csv("/content/IMDB Dataset.csv")  # Replace path if needed

# Initialize tools
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r"<.*?>", " ", text)           # remove HTML
    text = re.sub(r"[^a-z\s]", " ", text)        # remove non-letters
    tokens = nltk.word_tokenize(text)            # tokenize
    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]
    return " ".join(tokens)

df["clean_review"] = df["review"].apply(preprocess)
print(df.head())

df

all_words = " ".join(df["clean_review"]).split()
total_words = len(all_words)
unique_words = len(set(all_words))

print("Total words:", total_words)
print("Vocabulary size:", unique_words)

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(df["clean_review"])

one_hot = tokenizer.texts_to_matrix(df["clean_review"], mode="binary")
print(one_hot.shape)

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
X_bow = cv.fit_transform(df["clean_review"])

print("Vocabulary (BOW) size:", len(cv.vocabulary_))

# Show how many times each word occurred
word_counts = pd.DataFrame(
    X_bow.toarray().sum(axis=0),
    index=cv.get_feature_names_out(),
    columns=["count"]
).sort_values(by="count", ascending=False)

print(word_counts.head(20))

# Bi-gram model
cv2 = CountVectorizer(ngram_range=(2,2))
X_bi = cv2.fit_transform(df["clean_review"])
print("Bi-gram vocab size:", len(cv2.vocabulary_))

# Tri-gram model
cv3 = CountVectorizer(ngram_range=(3,3))
X_tri = cv3.fit_transform(df["clean_review"])
print("Tri-gram vocab size:", len(cv3.vocabulary_))

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(df["clean_review"])

print("TF-IDF vocab size:", len(tfidf.vocabulary_))

# Get IDF scores
idf_scores = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))

# Sort by lowest to highest idf
top_tfidf = sorted(idf_scores.items(), key=lambda x: x[1])
print(top_tfidf[:20])